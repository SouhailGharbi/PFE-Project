import torch 
from transformers import AutoTokenizer, T5ForConditionalGeneration
import spacy

# --- Chargement du mod√®le et du tokenizer ---
def load_model_and_tokenizer(model_path):
    """Charge le mod√®le et le tokenizer depuis le dossier sauvegard√©."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = T5ForConditionalGeneration.from_pretrained(model_path)
        return tokenizer, model
    except Exception as e:
        print(f"Erreur lors du chargement du mod√®le ou du tokenizer: {e}")
        return None, None

# --- Pr√©traitement de l'input avec spaCy ---
def preprocess_input_with_spacy(input_text):
    """Utilise spaCy pour analyser et traiter l'input en fran√ßais."""
    nlp = spacy.load("fr_core_news_sm")  # Charger le mod√®le spaCy pour le fran√ßais
    doc = nlp(input_text)  # Analyser le texte
    
    # Exemple de pr√©traitement : Lemmatization et extraction de tokens (vous pouvez ajouter plus de traitements ici)
    lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha]  # Lemmatization et filtrage des tokens alpha
    preprocessed_input = " ".join(lemmatized_tokens)
    
    # Affichage pour d√©bogage
    print("Texte pr√©trait√© avec spaCy :")
    print(preprocessed_input)
    
    return preprocessed_input

# --- G√©n√©ration de la requ√™te SQL ---
def generate_sql(prompt, tokenizer, model, device="cpu", max_length=128):
    """G√©n√®re une requ√™te SQL √† partir d'un prompt en fran√ßais."""
    # Pr√©traitement de l'input avec spaCy
    preprocessed_prompt = preprocess_input_with_spacy(prompt)
    
    # Formater le prompt pour le mod√®le (ajout des balises [NL] et [SQL])
    input_text = f"[NL] {preprocessed_prompt} [SQL]"

    # Tokenisation
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=max_length, padding="max_length")
    inputs = {key: val.to(device) for key, val in inputs.items()}  # Envoi sur le bon p√©riph√©rique (CPU/GPU)
    
    # Affichage des tokens pour d√©bogage (peut √™tre supprim√© une fois test√©)
    print("Tokens d'entr√©e :", inputs["input_ids"])
    print("Tokens d√©cod√©s :", tokenizer.convert_ids_to_tokens(inputs["input_ids"][0]))
    
    # G√©n√©ration de la requ√™te SQL
    with torch.no_grad():  # D√©sactive le calcul du gradient pour l'inf√©rence
        outputs = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_beams=5,  # Utiliser la recherche en faisceau
            early_stopping=True,  # Arr√™ter la g√©n√©ration si le mod√®le termine pr√©matur√©ment
            no_repeat_ngram_size=2,  # √âviter les r√©p√©titions de n-grammes
        )
    
    # D√©tokenisation
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return sql_query

# --- Script principal ---
def main():
    # Chemin vers le mod√®le sauvegard√©
    model_path = "saved_model"
    
    # Chargement du mod√®le et du tokenizer
    tokenizer, model = load_model_and_tokenizer(model_path)
    if tokenizer is None or model is None:
        return  # Si le mod√®le ou le tokenizer n'ont pas pu √™tre charg√©s, on arr√™te

    # Configuration du p√©riph√©rique (CPU ou GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # Prompt de test (question en fran√ßais)
    prompt = "Quels employ√©s travaillent dans le d√©partement 10 et gagnent plus de 5000‚Ç¨ ?"
    
    # G√©n√©ration de la requ√™te SQL
    sql_query = generate_sql(prompt, tokenizer, model, device)
    
    # Affichage des r√©sultats
    print(f"üîπ Prompt (Fran√ßais) : {prompt}")
    print(f"üîπ Requ√™te SQL G√©n√©r√©e : {sql_query}")

if __name__ == "__main__":
    main()
