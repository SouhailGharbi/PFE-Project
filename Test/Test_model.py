from transformers import AutoTokenizer, CamembertForSequenceClassification
import torch

# ğŸ‹ï¸ Charger le modÃ¨le et le tokenizer entraÃ®nÃ©s
MODEL_PATH = "chemin/vers/votre_modele"  # ğŸ“Œ Remplacez par le chemin rÃ©el
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = CamembertForSequenceClassification.from_pretrained(MODEL_PATH)

# Mettre le modÃ¨le en mode Ã©valuation
model.eval()

def predict_question(question):
    """Fait une prÃ©diction SQL Ã  partir d'une question en langage naturel."""
    input_text = f"Question: {question}"
    
    # Tokenisation
    tokens = tokenizer(input_text, truncation=True, padding="max_length", max_length=128, return_tensors="pt")

    input_ids = tokens["input_ids"]
    attention_mask = tokens["attention_mask"]

    # PrÃ©diction
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
    
    logits = outputs.logits
    predicted_label = torch.argmax(logits, dim=1).item()

    return predicted_label

# ğŸ”¥ Mode interactif dans le terminal
print("\nğŸ’¡ Tapez une question en langage naturel (ou 'exit' pour quitter) :")
while True:
    user_input = input("â“ Votre question : ")
    if user_input.lower() == "exit":
        print("ğŸ‘‹ Fin du test.")
        break

    prediction = predict_question(user_input)
    print(f"ğŸ¯ PrÃ©diction du modÃ¨le : {prediction}\n")
