from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# ğŸ“Œ Charger le modÃ¨le fine-tunÃ© et le tokenizer
model_name = "./fine_tuned_gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# ğŸ“Œ Ajouter le token de padding
tokenizer.pad_token = tokenizer.eos_token

# ğŸ“Œ Fonction pour gÃ©nÃ©rer une requÃªte SQL
def generate_sql(query):
    prompt = f"Question: {query}"  # Format structurÃ© \nRequÃªte SQL:
    
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=128)
    
    with torch.no_grad():  
        outputs = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],  
            max_length=100,  
            num_return_sequences=1,  
            pad_token_id=tokenizer.eos_token_id
        )

    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    #  Supprimer tout sauf la requÃªte SQL
    #if "RequÃªte SQL:" in generated_sql:
        #generated_sql = generated_sql.split("RequÃªte SQL:")[-1].strip()

    #  VÃ©rifier si le modÃ¨le a gÃ©nÃ©rÃ© du SQL valide
    if not generated_sql.strip().startswith("SELECT"):
        return " Le modÃ¨le n'a pas gÃ©nÃ©rÃ© une requÃªte SQL correcte."

    return generated_sql

#  Mode interactif
print("\nâœ… Tape une question en langage naturel pour gÃ©nÃ©rer une requÃªte SQL (ou 'exit' pour quitter) :")

while True:
    user_input = input("\nğŸ”¹ Question : ")
    if user_input.lower() == "exit":
        print("ğŸ‘‹ Fin du test.")
        break
    
    generated_sql = generate_sql(user_input)
    print(f"ğŸŸ¢ RequÃªte SQL gÃ©nÃ©rÃ©e : {generated_sql}")
