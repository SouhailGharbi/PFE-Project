import json
import os
import torch
from transformers import AutoTokenizer, CamembertForSequenceClassification

# ğŸ“Œ Charger les donnÃ©es d'entraÃ®nement
def load_sql_templates(train_data_path):
    """Charge le fichier JSON et crÃ©e un dictionnaire des requÃªtes SQL."""
    if not os.path.exists(train_data_path):
        print(f"âŒ Erreur : Le fichier {train_data_path} est introuvable.")
        return {}

    try:
        with open(train_data_path, "r", encoding="utf-8") as file:
            train_data = json.load(file)
        return {i: entry["output"] for i, entry in enumerate(train_data)}
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture de {train_data_path} : {e}")
        return {}

# ğŸ“Œ Charger le modÃ¨le et le tokenizer
def load_model_and_tokenizer(model_path):
    """Charge le modÃ¨le et le tokenizer entraÃ®nÃ©s."""
    if not os.path.exists(model_path):
        print(f"âŒ Erreur : Le modÃ¨le '{model_path}' est introuvable.")
        return None, None

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = CamembertForSequenceClassification.from_pretrained(model_path)
        model.eval()
        return model, tokenizer
    except Exception as e:
        print(f"âŒ Erreur lors du chargement du modÃ¨le : {e}")
        return None, None

# ğŸ“Œ Tokenisation
def tokenize_question(question, tokenizer, max_length=128):
    """Tokenise la question et gÃ¨re les erreurs Ã©ventuelles."""
    try:
        tokens = tokenizer(question, truncation=True, max_length=max_length, padding="longest", return_tensors="pt")
        return tokens["input_ids"], tokens["attention_mask"]
    except Exception as e:
        print(f"âŒ Erreur de tokenisation : {e}")
        return None, None

# ğŸ“Œ PrÃ©diction
def predict(model, tokenizer, question):
    """Utilise le modÃ¨le pour prÃ©dire la classe correspondant Ã  la question."""
    input_ids, attention_mask = tokenize_question(question, tokenizer)
    if input_ids is None or attention_mask is None:
        return None

    try:
        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
        return torch.argmax(outputs.logits, dim=1).item()
    except Exception as e:
        print(f"âŒ Erreur de prÃ©diction : {e}")
        return None

# ğŸ“Œ GÃ©nÃ©rer la requÃªte SQL
def generate_sql(prediction, sql_templates):
    """Retourne la requÃªte SQL correspondant Ã  la classe prÃ©dite."""
    return sql_templates.get(prediction, "âš ï¸ Aucune requÃªte trouvÃ©e.") if prediction is not None else "âš ï¸ Erreur dans la prÃ©diction."

# ğŸ“Œ Fonction principale
def main():
    model_path = r"D:\PFE Project\Train\saved_model"
    train_data_path = r"D:\PFE Project\Train\Train_Data.json"

    sql_templates = load_sql_templates(train_data_path)
    if not sql_templates:
        print("âŒ Impossible de charger les requÃªtes SQL. VÃ©rifiez le fichier JSON.")
        return

    model, tokenizer = load_model_and_tokenizer(model_path)
    if model is None or tokenizer is None:
        print("âŒ Impossible de charger le modÃ¨le. VÃ©rifiez le chemin.")
        return

    while True:
        question = input("ğŸ’¬ Entrez une question SQL (ou 'exit' pour quitter) : ")
        if question.lower() == "exit":
            break

        prediction = predict(model, tokenizer, question)
        sql_query = generate_sql(prediction, sql_templates)

        print("\nğŸ“ Question SQL :", question)
        print("ğŸ“Œ RequÃªte SQL gÃ©nÃ©rÃ©e :", sql_query, "\n")

# ExÃ©cuter le test
if __name__ == "__main__":
    main()
