from transformers import AutoTokenizer
import json
import re

def load_dataset(file_path):
    """Charge le dataset JSON √† partir du fichier donn√©."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return json.load(file)
    except FileNotFoundError:
        print(f"‚ùå Erreur : Fichier {file_path} introuvable.")
        return None
    except json.JSONDecodeError:
        print("‚ùå Erreur : Format JSON invalide.")
        return None

def extract_sql_identifiers(dataset):
    """Extrait les noms de tables et colonnes SQL du dataset."""
    identifiers = set()
    sql_pattern = re.compile(r"\b[A-Za-z_][A-Za-z0-9_]*\b")  # D√©tecte mots SQL # Expression r√©guli√®re pour capturer les noms de colonnes/tables

    sql_keywords = {
        "SELECT", "FROM", "WHERE", "AND", "OR", "IN", "LIKE", "JOIN",
        "ORDER", "GROUP", "BY", "HAVING", "COUNT", "SUM", "AVG", "MIN",
        "MAX", "INSERT", "UPDATE", "DELETE", "CREATE", "ALTER", "DROP",
        "TABLE", "DATABASE"
    }
    for data in dataset:
        if 'output' in data: # V√©rifie que la cl√© 'output' est pr√©sente
            sql_query = data['output']
            words = sql_pattern.findall(sql_query) # Extraction des mots dans la requ√™te SQL
            normalized_query = " ".join([word.upper() if word.upper() in sql_keywords else word for word in words])
            data['output'] = normalized_query  # Mettre √† jour la requ√™te SQL normalis√©e

            # Filtrer pour √©viter d'ajouter des mots-cl√©s SQL comme tokens
            for word in words:
                if word.upper() not in ["SELECT", "FROM", "WHERE", "AND", "OR", "IN", "LIKE", "JOIN",
                                        "ORDER", "GROUP", "BY", "HAVING", "COUNT", "SUM", "AVG", "MIN",
                                        "MAX", "INSERT", "UPDATE", "DELETE", "CREATE", "ALTER", "DROP",
                                        "TABLE", "DATABASE"]:
                    identifiers.add(word)

    return list(identifiers)

def initialize_tokenizer(dataset):
    """Initialise le tokenizer CamemBERT et ajoute les tokens SQL + colonnes/tables."""
    tokenizer = AutoTokenizer.from_pretrained("camembert-base")

    # Ajouter les mots-cl√©s SQL
    sql_tokens = [
        "SELECT", "FROM", "WHERE", "AND", "OR", "NOT", "IN", "LIKE", "JOIN",
        "ORDER BY", "GROUP BY", "HAVING", "COUNT", "SUM", "AVG", "MIN", "MAX",
        "INSERT", "UPDATE", "DELETE", "CREATE", "ALTER", "DROP", "TABLE", "DATABASE"
    ]

    # Ajouter les noms de tables et colonnes trouv√©s dynamiquement
    table_column_tokens = extract_sql_identifiers(dataset)
    all_tokens = sql_tokens + table_column_tokens
    tokenizer.add_tokens(all_tokens)

    tokenizer.pad_token = tokenizer.eos_token
    return tokenizer

def tokenize_dataset(dataset, tokenizer):
    """Tokenise les donn√©es et retourne une liste de tensors."""
    tokenized_dataset = []
    
    for i, data in enumerate(dataset):
        if 'input' not in data or 'output' not in data:
            print(f"‚ö†Ô∏è Donn√©e ignor√©e √† l'index {i} : cl√© 'input' ou 'output' manquante.")
            continue
        
        input_text = f"Question: {data['input']} R√©ponse: {data['output']}"
        tokenized = tokenizer(
            input_text, 
            truncation=True, 
            padding=True,  
            return_tensors="pt",
            add_special_tokens=True
        )
        
        tokenized_dataset.append({
            "input_text": input_text, 
            "input_ids": tokenized["input_ids"], 
            "attention_mask": tokenized["attention_mask"]
        })
        
        if i % 100 == 0:
            print(f"‚è≥ Tokenisation en cours... {i}/{len(dataset)}")

    print("‚úÖ Tokenisation termin√©e.")
    return tokenized_dataset

def main():
    """Ex√©cution principale du script."""
    dataset_path = "Train/Train_Data.json"
    dataset = load_dataset(dataset_path)
    
    if dataset is None:
        return

    tokenizer = initialize_tokenizer(dataset)
    tokenized_data = tokenize_dataset(dataset, tokenizer)

    # Affichage du premier √©l√©ment tokenis√©
    if tokenized_data:
        print("\nüîç Exemple de donn√©e tokenis√©e :")
        print(f"Texte d'origine : {tokenized_data[0]['input_text']}")
        print(f"Tokens : {tokenizer.convert_ids_to_tokens(tokenized_data[0]['input_ids'][0])}")
        print(f"IDs : {tokenized_data[0]['input_ids']}")
        print(f"Masque d'attention : {tokenized_data[0]['attention_mask']}")

if __name__ == "__main__":
    main()
